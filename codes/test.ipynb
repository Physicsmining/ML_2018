{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from prepare import *\n",
    "from costs import *\n",
    "from grid_search import *\n",
    "from gradient_descent import *\n",
    "from stochastic_gradient_descent import *\n",
    "from build_polynomial import *\n",
    "from least_squares import *\n",
    "from split_data import *\n",
    "from ridge_regression import * \n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from helpers import *\n",
    "\n",
    "height, weight, gender = load_data_from_ex02(sub_sample=False, add_outlier=False)\n",
    "x, mean_x, std_x = standardize(height)\n",
    "y, tx = build_model_data(x, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000,), (10000, 2))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, tx.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Search: loss*=42.42448314678248, w0*=66.66666666666669, w1*=16.666666666666686, execution time=0.106 seconds\n"
     ]
    }
   ],
   "source": [
    "#from grid_search import generate_w, get_best_parameters\n",
    "#from plots import grid_visualization\n",
    "\n",
    "# Generate the grid of parameters to be swept\n",
    "grid_w0, grid_w1 = generate_w(num_intervals=10)\n",
    "\n",
    "# Start the grid search\n",
    "start_time = datetime.datetime.now()\n",
    "grid_losses = grid_search(y, tx, grid_w0, grid_w1)\n",
    "\n",
    "# Select the best combinaison\n",
    "loss_star, w0_star, w1_star = get_best_parameters(grid_w0, grid_w1, grid_losses)\n",
    "end_time = datetime.datetime.now()\n",
    "execution_time = (end_time - start_time).total_seconds()\n",
    "\n",
    "# Print the results\n",
    "print(\"Grid Search: loss*={l}, w0*={w0}, w1*={w1}, execution time={t:.3f} seconds\".format(\n",
    "      l=loss_star, w0=w0_star, w1=w1_star, t=execution_time))\n",
    "\n",
    "# Plot the results\n",
    "#fig = grid_visualization(grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight)\n",
    "#fig.set_size_inches(10.0,6.0)\n",
    "#fig.savefig(\"grid_plot\")  # Optional saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, please fill in the functions `compute_gradient` below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please fill in the functions `gradient_descent` below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your gradient descent function through gradient descent demo shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=2792.2367127591674, w0=51.30574540147352, w1=9.435798704492393\n",
      "Gradient Descent(1/49): loss=265.302462108962, w0=66.69746902191565, w1=12.266538315840034\n",
      "Gradient Descent(2/49): loss=37.87837955044161, w0=71.31498610804833, w1=13.115760199244338\n",
      "Gradient Descent(3/49): loss=17.410212120174496, w0=72.70024123388814, w1=13.370526764265632\n",
      "Gradient Descent(4/49): loss=15.568077051450455, w0=73.11581777164008, w1=13.446956733772023\n",
      "Gradient Descent(5/49): loss=15.402284895265295, w0=73.24049073296567, w1=13.469885724623941\n",
      "Gradient Descent(6/49): loss=15.38736360120863, w0=73.27789262136332, w1=13.476764421879517\n",
      "Gradient Descent(7/49): loss=15.38602068474353, w0=73.28911318788263, w1=13.478828031056189\n",
      "Gradient Descent(8/49): loss=15.385899822261674, w0=73.29247935783842, w1=13.47944711380919\n",
      "Gradient Descent(9/49): loss=15.385888944638305, w0=73.29348920882516, w1=13.47963283863509\n",
      "Gradient Descent(10/49): loss=15.3858879656522, w0=73.29379216412119, w1=13.479688556082861\n",
      "Gradient Descent(11/49): loss=15.385887877543453, w0=73.29388305071, w1=13.479705271317192\n",
      "Gradient Descent(12/49): loss=15.385887869613667, w0=73.29391031668663, w1=13.479710285887492\n",
      "Gradient Descent(13/49): loss=15.385887868899983, w0=73.29391849647962, w1=13.479711790258582\n",
      "Gradient Descent(14/49): loss=15.38588786883575, w0=73.29392095041752, w1=13.479712241569908\n",
      "Gradient Descent(15/49): loss=15.385887868829974, w0=73.29392168659889, w1=13.479712376963306\n",
      "Gradient Descent(16/49): loss=15.38588786882945, w0=73.2939219074533, w1=13.479712417581325\n",
      "Gradient Descent(17/49): loss=15.385887868829403, w0=73.29392197370962, w1=13.479712429766732\n",
      "Gradient Descent(18/49): loss=15.3858878688294, w0=73.29392199358651, w1=13.479712433422353\n",
      "Gradient Descent(19/49): loss=15.385887868829398, w0=73.29392199954958, w1=13.47971243451904\n",
      "Gradient Descent(20/49): loss=15.385887868829398, w0=73.29392200133852, w1=13.479712434848047\n",
      "Gradient Descent(21/49): loss=15.3858878688294, w0=73.29392200187519, w1=13.479712434946748\n",
      "Gradient Descent(22/49): loss=15.3858878688294, w0=73.29392200203618, w1=13.479712434976358\n",
      "Gradient Descent(23/49): loss=15.3858878688294, w0=73.29392200208449, w1=13.479712434985242\n",
      "Gradient Descent(24/49): loss=15.3858878688294, w0=73.29392200209898, w1=13.479712434987906\n",
      "Gradient Descent(25/49): loss=15.385887868829398, w0=73.29392200210333, w1=13.479712434988706\n",
      "Gradient Descent(26/49): loss=15.3858878688294, w0=73.29392200210462, w1=13.479712434988945\n",
      "Gradient Descent(27/49): loss=15.3858878688294, w0=73.29392200210502, w1=13.479712434989018\n",
      "Gradient Descent(28/49): loss=15.3858878688294, w0=73.29392200210515, w1=13.47971243498904\n",
      "Gradient Descent(29/49): loss=15.3858878688294, w0=73.29392200210518, w1=13.479712434989047\n",
      "Gradient Descent(30/49): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(31/49): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(32/49): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(33/49): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(34/49): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(35/49): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(36/49): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(37/49): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(38/49): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(39/49): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(40/49): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(41/49): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(42/49): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(43/49): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(44/49): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(45/49): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(46/49): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(47/49): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(48/49): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent(49/49): loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "Gradient Descent: execution time=0.037 seconds\n"
     ]
    }
   ],
   "source": [
    "# from gradient_descent import *\n",
    "#from plots import gradient_descent_visualization\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.7\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "gradient_losses, gradient_ws = gradient_descent(y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Gradient Descent: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD(0/49): loss=439.7872871106584, w0=51.79729490970891, w1=33.144346225988336\n",
      "SGD(1/49): loss=125.89874008305473, w0=79.84217783915867, w1=0.1325760409551009\n",
      "SGD(2/49): loss=157.5088606629287, w0=89.95378816480684, w1=16.06714456531969\n",
      "SGD(3/49): loss=104.72073145071103, w0=80.30353031165458, w1=24.86106026863597\n",
      "SGD(4/49): loss=151.2178665811003, w0=64.85216233046326, w1=-0.6765812561852762\n",
      "SGD(5/49): loss=36.84822334154387, w0=79.84470434989466, w1=13.37052645915481\n",
      "SGD(6/49): loss=24.487666374703853, w0=76.339597809916, w1=10.491834277254894\n",
      "SGD(7/49): loss=17.48306158050214, w0=71.26984202177543, w1=13.791878507094536\n",
      "SGD(8/49): loss=16.55294218223241, w0=71.77545147075095, w1=13.648104454440093\n",
      "SGD(9/49): loss=70.51408604915358, w0=80.63667780395893, w1=20.985732258769936\n",
      "SGD(10/49): loss=97.27964145410704, w0=74.58779446116539, w1=26.212088538067405\n",
      "SGD(11/49): loss=93.24106689021808, w0=75.6024661091101, w1=25.742704816491628\n",
      "SGD(12/49): loss=255.61063196923783, w0=57.499567522112386, w1=-1.7185721643053284\n",
      "SGD(13/49): loss=411.3770336286922, w0=85.4706891309507, w1=38.85112614825291\n",
      "SGD(14/49): loss=152.2777604422043, w0=71.94258219346958, w1=29.970850232859345\n",
      "SGD(15/49): loss=109.18658153101778, w0=62.169718479975025, w1=21.470549895643806\n",
      "SGD(16/49): loss=142.43040260220693, w0=69.82370643767452, w1=29.037560403237677\n",
      "SGD(17/49): loss=135.55709884380292, w0=75.41829486510626, w1=28.836452384890254\n",
      "SGD(18/49): loss=255.87194263561642, w0=91.81082820386251, w1=1.7282744698336927\n",
      "SGD(19/49): loss=224.33850276041892, w0=87.49074513032937, w1=-1.2293134623672244\n",
      "SGD(20/49): loss=164.83712997397504, w0=82.93515128792484, w1=-0.8712172236966371\n",
      "SGD(21/49): loss=125.88223869806112, w0=78.42093414382694, w1=-0.4740128080097184\n",
      "SGD(22/49): loss=107.42568050462818, w0=74.49299138837335, w1=-0.03479154697462483\n",
      "SGD(23/49): loss=98.06109366409981, w0=61.35097888475389, w1=8.71352724279894\n",
      "SGD(24/49): loss=105.02233592829751, w0=63.53305656211548, w1=4.314648224556535\n",
      "SGD(25/49): loss=136.87260601365492, w0=80.52644019824166, w1=27.28783022244314\n",
      "SGD(26/49): loss=57.619670776556895, w0=73.63732770461172, w1=22.663918352701312\n",
      "SGD(27/49): loss=84.78875007458899, w0=66.89041515925298, w1=23.3691423743265\n",
      "SGD(28/49): loss=40.595372174739396, w0=70.80242429119632, w1=20.128878380305483\n",
      "SGD(29/49): loss=32.11817800780985, w0=73.49607429934709, w1=19.261037086544324\n",
      "SGD(30/49): loss=37.070769647181315, w0=74.34432062993554, w1=19.98097587214924\n",
      "SGD(31/49): loss=50.34535494667709, w0=76.52568093153856, w1=21.191694565266783\n",
      "SGD(32/49): loss=40.82433473901351, w0=73.507639399321, w1=20.609314009726273\n",
      "SGD(33/49): loss=32.183901451264774, w0=67.56813139884686, w1=14.380461641210923\n",
      "SGD(34/49): loss=22.98749574899363, w0=70.26716205021506, w1=11.02167664314985\n",
      "SGD(35/49): loss=21.25380534929885, w0=74.78001472252525, w1=10.39306969776574\n",
      "SGD(36/49): loss=22.47789728556828, w0=70.29930507925239, w1=11.195792927028792\n",
      "SGD(37/49): loss=21.03083230819113, w0=70.79608853636095, w1=11.232332436828608\n",
      "SGD(38/49): loss=56.87608652957934, w0=79.19520820978425, w1=6.540316321741399\n",
      "SGD(39/49): loss=73.36729267111525, w0=66.72424523457535, w1=22.01213042608338\n",
      "SGD(40/49): loss=44.70643952003933, w0=76.76637548851073, w1=6.654525984263341\n",
      "SGD(41/49): loss=61.2288749641971, w0=80.52933656847166, w1=7.207970233371772\n",
      "SGD(42/49): loss=56.21618926344991, w0=79.31807085826212, w1=6.743969465716535\n",
      "SGD(43/49): loss=54.511398651884555, w0=74.98435305195311, w1=4.796771433702657\n",
      "SGD(44/49): loss=227.70874333454807, w0=62.09598121641495, w1=30.778609309981164\n",
      "SGD(45/49): loss=37.31531440044867, w0=79.74891772759023, w1=14.960213447217168\n",
      "SGD(46/49): loss=24.25316907613787, w0=75.99412993124426, w1=16.711343533061886\n",
      "SGD(47/49): loss=18.475242358446803, w0=71.12725648724751, w1=12.261406428289044\n",
      "SGD(48/49): loss=19.29050984514458, w0=70.84118262873734, w1=12.140565871329388\n",
      "SGD(49/49): loss=20.230201301201774, w0=70.31539536771265, w1=12.575828618915862\n",
      "SGD: execution time=0.060 seconds\n"
     ]
    }
   ],
   "source": [
    "# from stochastic_gradient_descent import *\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.7\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SGD.\n",
    "start_time = datetime.datetime.now()\n",
    "sgd_losses, sgd_ws = stochastic_gradient_descent(\n",
    "    y, tx, w_initial, batch_size, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Least square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7.32839801e+01, 1.34792217e+01, 9.94194214e-03])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "degree = 2\n",
    "tx = build_poly(x, degree)\n",
    "weights = least_squares(y, tx)\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of x (50,)\n",
      "shape of y (50,)\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "x, y = load_data()\n",
    "print(\"shape of x {}\".format(x.shape))\n",
    "print(\"shape of y {}\".format(y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split_demo(x, y, degree, ratio, seed):\n",
    "    \"\"\"polynomial regression with different split ratios and different degrees.\"\"\"\n",
    "    x_tr, y_tr, x_te, y_te = split_data(x, y, ratio, seed)\n",
    "    # form tx\n",
    "    tx_tr = build_poly(x_tr, degree)\n",
    "    tx_te = build_poly(x_te, degree)\n",
    "    \n",
    "    print(y_tr.shape,tx_tr.shape)\n",
    "\n",
    "    weight = least_squares(y_tr, tx_tr)\n",
    "    \n",
    "\n",
    "    # calculate RMSE for train and test data.\n",
    "    rmse_tr = np.sqrt(2 * compute_mse(y_tr, tx_tr, weight))\n",
    "    rmse_te = np.sqrt(2 * compute_mse(y_te, tx_te, weight))\n",
    "\n",
    "    print(\"proportion={p}, degree={d}, Training RMSE={tr:.3f}, Testing RMSE={te:.3f}\".format(\n",
    "          p=ratio, d=degree, tr=rmse_tr, te=rmse_te))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45,) (45, 2)\n",
      "proportion=0.9, degree=1, Training RMSE=0.494, Testing RMSE=0.181\n",
      "(45,) (45, 4)\n",
      "proportion=0.9, degree=3, Training RMSE=0.264, Testing RMSE=0.206\n",
      "(45,) (45, 8)\n",
      "proportion=0.9, degree=7, Training RMSE=0.254, Testing RMSE=0.220\n",
      "(45,) (45, 13)\n",
      "proportion=0.9, degree=12, Training RMSE=0.242, Testing RMSE=0.250\n",
      "(25,) (25, 2)\n",
      "proportion=0.5, degree=1, Training RMSE=0.455, Testing RMSE=0.531\n",
      "(25,) (25, 4)\n",
      "proportion=0.5, degree=3, Training RMSE=0.239, Testing RMSE=0.296\n",
      "(25,) (25, 8)\n",
      "proportion=0.5, degree=7, Training RMSE=0.232, Testing RMSE=0.284\n",
      "(25,) (25, 13)\n",
      "proportion=0.5, degree=12, Training RMSE=0.205, Testing RMSE=1.548\n",
      "(5,) (5, 2)\n",
      "proportion=0.1, degree=1, Training RMSE=0.428, Testing RMSE=0.534\n",
      "(5,) (5, 4)\n",
      "proportion=0.1, degree=3, Training RMSE=0.085, Testing RMSE=0.460\n",
      "(5,) (5, 8)\n",
      "proportion=0.1, degree=7, Training RMSE=0.000, Testing RMSE=2.254\n",
      "(5,) (5, 13)\n",
      "proportion=0.1, degree=12, Training RMSE=0.000, Testing RMSE=4.651\n"
     ]
    }
   ],
   "source": [
    "seed = 6\n",
    "degrees = [1, 3, 7, 12]\n",
    "split_ratios = [0.9, 0.5, 0.1]\n",
    "\n",
    "for split_ratio in split_ratios:\n",
    "    for degree in degrees:\n",
    "        train_test_split_demo(x, y, degree, split_ratio, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression_demo(x, y, degree, ratio, seed):\n",
    "    \"\"\"ridge regression demo.\"\"\"\n",
    "    # define parameter\n",
    "    lambdas = np.logspace(-5, 0, 15)\n",
    "    # split data\n",
    "    x_tr, x_te, y_tr, y_te = split_data(x, y, ratio, seed)\n",
    "    # form tx\n",
    "    tx_tr = build_poly(x_tr, degree)\n",
    "    tx_te = build_poly(x_te, degree)\n",
    "\n",
    "    # ridge regression with different lambda\n",
    "    rmse_tr = []\n",
    "    rmse_te = []\n",
    "    for ind, lambda_ in enumerate(lambdas):\n",
    "        # ridge regression\n",
    "        weight = ridge_regression(y_tr, tx_tr, lambda_)\n",
    "        rmse_tr.append(np.sqrt(2 * compute_mse(y_tr, tx_tr, weight)))\n",
    "        rmse_te.append(np.sqrt(2 * compute_mse(y_te, tx_te, weight)))\n",
    "\n",
    "        print(\"proportion={p}, degree={d}, lambda={l:.3f}, Training RMSE={tr:.3f}, Testing RMSE={te:.3f}\".format(\n",
    "               p=ratio, d=degree, l=lambda_, tr=rmse_tr[ind], te=rmse_te[ind]))\n",
    "    #plot_train_test(rmse_tr, rmse_te, lambdas, degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proportion=0.5, degree=7, lambda=0.000, Training RMSE=1.299, Testing RMSE=67.418\n",
      "proportion=0.5, degree=7, lambda=0.000, Training RMSE=1.299, Testing RMSE=59.973\n",
      "proportion=0.5, degree=7, lambda=0.000, Training RMSE=1.299, Testing RMSE=48.615\n",
      "proportion=0.5, degree=7, lambda=0.000, Training RMSE=1.300, Testing RMSE=35.581\n",
      "proportion=0.5, degree=7, lambda=0.000, Training RMSE=1.301, Testing RMSE=24.837\n",
      "proportion=0.5, degree=7, lambda=0.001, Training RMSE=1.301, Testing RMSE=18.204\n",
      "proportion=0.5, degree=7, lambda=0.001, Training RMSE=1.302, Testing RMSE=14.639\n",
      "proportion=0.5, degree=7, lambda=0.003, Training RMSE=1.304, Testing RMSE=12.391\n",
      "proportion=0.5, degree=7, lambda=0.007, Training RMSE=1.312, Testing RMSE=10.274\n",
      "proportion=0.5, degree=7, lambda=0.016, Training RMSE=1.329, Testing RMSE=7.893\n",
      "proportion=0.5, degree=7, lambda=0.037, Training RMSE=1.357, Testing RMSE=5.613\n",
      "proportion=0.5, degree=7, lambda=0.085, Training RMSE=1.384, Testing RMSE=3.947\n",
      "proportion=0.5, degree=7, lambda=0.193, Training RMSE=1.405, Testing RMSE=2.893\n",
      "proportion=0.5, degree=7, lambda=0.439, Training RMSE=1.422, Testing RMSE=2.145\n",
      "proportion=0.5, degree=7, lambda=1.000, Training RMSE=1.442, Testing RMSE=1.556\n"
     ]
    }
   ],
   "source": [
    "seed = 56\n",
    "degree = 7\n",
    "split_ratio = 0.5\n",
    "ridge_regression_demo(x, y, degree, split_ratio, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "widgets": {
   "state": {
    "d2b2c3aea192430e81437f33ba0b0e69": {
     "views": [
      {
       "cell_index": 22
      }
     ]
    },
    "e4a6a7a70ccd42ddb112989c04f2ed3f": {
     "views": [
      {
       "cell_index": 18
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
