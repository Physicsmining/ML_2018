{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start loading data..."
     ]
    },
    {
     "ename": "OSError",
     "evalue": "data/train.csv not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a2927f8f054b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mdata_path_tr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'data/train.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mdata_path_te\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'data/test.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0myb_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_csv_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_sample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0myb_te\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_te\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_te\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m      \u001b[0;34m=\u001b[0m \u001b[0mload_csv_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path_te\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_sample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Files/Semester_1/Machine Learning/ML_Project/Project_1/ML_2018/helpers.py\u001b[0m in \u001b[0;36mload_csv_data\u001b[0;34m(data_path, sub_sample)\u001b[0m\n\u001b[1;32m     14\u001b[0m              \u001b[0mlabels\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mstring\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m \u001b[0mname\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \"\"\"\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenfromtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_header\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musecols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenfromtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_header\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mgenfromtxt\u001b[0;34m(fname, dtype, comments, delimiter, skip_header, skip_footer, converters, missing_values, filling_values, usecols, names, excludelist, deletechars, replace_space, autostrip, case_sensitive, defaultfmt, unpack, usemask, loose, invalid_raise, max_rows, encoding)\u001b[0m\n\u001b[1;32m   1687\u001b[0m             \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1688\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1689\u001b[0;31m             \u001b[0mfhd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_datasource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1690\u001b[0m             \u001b[0mown_fhd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/numpy/lib/_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(path, mode, destpath, encoding, newline)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataSource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdestpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnewline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/numpy/lib/_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, path, mode, encoding, newline)\u001b[0m\n\u001b[1;32m    614\u001b[0m                                       encoding=encoding, newline=newline)\n\u001b[1;32m    615\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 616\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s not found.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    617\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: data/train.csv not found."
     ]
    }
   ],
   "source": [
    "# import basic packages\n",
    "import numpy as np\n",
    "# import self-defined modules\n",
    "from implementations import *\n",
    "from tools import *\n",
    "from helpers import *\n",
    "# just to ingore warning\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# -------------------- load train and test data -------------------- #\n",
    "print('Start loading data...',end='')\n",
    "data_path_tr = 'data/train.csv'\n",
    "data_path_te = 'data/test.csv'\n",
    "yb_tr, data_tr, idx_tr, labels = load_csv_data(data_path_tr, sub_sample=False)\n",
    "yb_te, data_te, idx_te, _      = load_csv_data(data_path_te, sub_sample=False)\n",
    "\n",
    "# prepare the data using self-defined DataFrame class\n",
    "labels_dataframe = ['Prediction'] + labels\n",
    "data_tr_dataframe = np.concatenate((yb_tr.reshape([-1, 1]), data_tr), axis=1)\n",
    "data_te_dataframe = np.concatenate((yb_te.reshape([-1, 1]), data_te), axis=1)\n",
    "dataframe_tr = DataFrame(data_tr_dataframe, idx_tr.tolist(), labels_dataframe)\n",
    "dataframe_te = DataFrame(data_te_dataframe, idx_te.tolist(), labels_dataframe)\n",
    "print('Completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# -------------------- data processing -------------------- #\n",
    "# replace missing values with mode\n",
    "print('Start processing data...',end='')\n",
    "DER_mode_s_tr = 119.89\n",
    "DER_mode_b_tr = 96.819\n",
    "temp = dataframe_tr.loc(dataframe_tr['DER_mass_MMC']==-999)\n",
    "temp = temp.loc(temp['Prediction']==1)\n",
    "dataframe_tr.loc(temp.index)['DER_mass_MMC'] = DER_mode_s_tr\n",
    "temp = dataframe_tr.loc(dataframe_tr['DER_mass_MMC']==-999)\n",
    "temp = temp.loc(temp['Prediction']==-1)\n",
    "dataframe_tr.loc(temp.index)['DER_mass_MMC'] = DER_mode_b_tr\n",
    "\n",
    "DER_mode_te = 96.728\n",
    "temp = dataframe_te.loc(dataframe_te['DER_mass_MMC']==-999)\n",
    "dataframe_te.loc(temp.index)['DER_mass_MMC'] = DER_mode_te\n",
    "\n",
    "# drop features\n",
    "feature_dorp_phi = ['PRI_jet_leading_phi', \n",
    "                    'PRI_jet_subleading_phi', \n",
    "                    'PRI_lep_phi', \n",
    "                    'PRI_met_phi', \n",
    "                    'PRI_tau_phi']\n",
    "dataframe_tr = dataframe_tr.drop(feature_dorp_phi)\n",
    "dataframe_te = dataframe_te.drop(feature_dorp_phi)\n",
    "# divide training data to 3 groups according to feature 'PRI_jet_num'\n",
    "def group_features_by_jet(dataframe):\n",
    "    return {  \n",
    "        0: dataframe.loc( dataframe['PRI_jet_num'] == 0).copy(),\n",
    "        1: dataframe.loc( dataframe['PRI_jet_num'] == 1).copy(),\n",
    "        2: dataframe.loc((dataframe['PRI_jet_num'] == 2) | (dataframe['PRI_jet_num'] == 3)).copy()}\n",
    "dataframe_tr_grp = group_features_by_jet(dataframe_tr)\n",
    "dataframe_te_grp = group_features_by_jet(dataframe_te)\n",
    "# drop features with undefined values (features whose missing rate of -999 are 100%)\n",
    "feature_undefined_gp0 = ['DER_deltaeta_jet_jet',\n",
    "                         'DER_mass_jet_jet',\n",
    "                         'DER_prodeta_jet_jet',\n",
    "                         'DER_lep_eta_centrality',\n",
    "                         'PRI_jet_leading_pt',\n",
    "                         'PRI_jet_leading_eta',\n",
    "                         'PRI_jet_subleading_pt',\n",
    "                         'PRI_jet_subleading_eta']\n",
    "feature_undefined_gp1 = ['DER_deltaeta_jet_jet',\n",
    "                         'DER_mass_jet_jet',\n",
    "                         'DER_prodeta_jet_jet',\n",
    "                         'DER_lep_eta_centrality',\n",
    "                         'PRI_jet_subleading_pt',\n",
    "                         'PRI_jet_subleading_eta']\n",
    "dataframe_tr_grp[0] = dataframe_tr_grp[0].drop(feature_undefined_gp0)\n",
    "dataframe_tr_grp[1] = dataframe_tr_grp[1].drop(feature_undefined_gp1)\n",
    "dataframe_te_grp[0] = dataframe_te_grp[0].drop(feature_undefined_gp0)\n",
    "dataframe_te_grp[1] = dataframe_te_grp[1].drop(feature_undefined_gp1)\n",
    "# drop feature 'PRI_jet_num' which is already used for grouping\n",
    "dataframe_tr_grp[0] = dataframe_tr_grp[0].drop('PRI_jet_num')\n",
    "dataframe_tr_grp[1] = dataframe_tr_grp[1].drop('PRI_jet_num')\n",
    "dataframe_tr_grp[2] = dataframe_tr_grp[2].drop('PRI_jet_num')\n",
    "dataframe_te_grp[0] = dataframe_te_grp[0].drop('PRI_jet_num')\n",
    "dataframe_te_grp[1] = dataframe_te_grp[1].drop('PRI_jet_num')\n",
    "dataframe_te_grp[2] = dataframe_te_grp[2].drop('PRI_jet_num')\n",
    "# drop features with 0 values \n",
    "dataframe_tr_grp[0] = dataframe_tr_grp[0].drop('PRI_jet_all_pt')\n",
    "dataframe_te_grp[0] = dataframe_te_grp[0].drop('PRI_jet_all_pt')\n",
    "# get the finally data\n",
    "data_tr_grp = []     \n",
    "pred_tr_grp = []     \n",
    "for index in range(len(dataframe_tr_grp)):\n",
    "    data_tr_grp.append(dataframe_tr_grp[index].drop('Prediction').values)\n",
    "    pred_tr_grp.append((dataframe_tr_grp[index])['Prediction'])\n",
    "data_te_grp = []     \n",
    "pred_te_grp = []     \n",
    "for index in range(len(dataframe_te_grp)):\n",
    "    data_te_grp.append(dataframe_te_grp[index].drop('Prediction').values)\n",
    "    pred_te_grp.append((dataframe_te_grp[index])['Prediction'])    \n",
    "print('Completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- model training -------------------- #\n",
    "# use ridge regression model\n",
    "print('Start training...', end='')    \n",
    "lambda_ = [0.001, 0.0005, 0.0005]\n",
    "degrees = [9,11,13]\n",
    "k_fold = 10\n",
    "seed = 10\n",
    "y_te = []\n",
    "w = []\n",
    "for i in range(len(data_tr_grp)):\n",
    "    # model training\n",
    "    x = data_tr_grp[i]\n",
    "    x = log_process(x)\n",
    "    x, _ ,_ = standardize(x)\n",
    "    x_tr = build_poly(x,degrees[i])\n",
    "    y_tr = pred_tr_grp[i]\n",
    "    w_tmp, _, _  = cv_loop(y_tr, x_tr, k_fold , seed, ridge_regression, lambda_=lambda_[i])\n",
    "    w.append(w_tmp)\n",
    "    # test model\n",
    "    x = data_te_grp[i]\n",
    "    x = log_process(x)\n",
    "    x, _, _ = standardize(x)\n",
    "    x_te = build_poly(x, degrees[i])\n",
    "    y_te.append(predict_labels(w[i], x_te))\n",
    "    \n",
    "y_pred_te = np.concatenate((y_te[0], y_te[1], y_te[2]))\n",
    "idx_te = np.concatenate((np.array(dataframe_te_grp[0].index),\n",
    "                         np.array(dataframe_te_grp[1].index), \n",
    "                         np.array(dataframe_te_grp[2].index)))\n",
    "print('Completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- generate prediction files -------------------- #\n",
    "print('Start generating prediction files...', end='')\n",
    "output_path = 'data/final.csv'\n",
    "create_csv_submission(idx_te, y_pred_te, output_path)\n",
    "print('Completed')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
