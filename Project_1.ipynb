{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from implementations import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 138.47 ,   51.655,   97.827, ...,    1.24 ,   -2.475,  113.497],\n",
       "       [ 160.937,   68.768,  103.235, ..., -999.   , -999.   ,   46.226],\n",
       "       [-999.   ,  162.172,  125.953, ..., -999.   , -999.   ,   44.251],\n",
       "       ...,\n",
       "       [ 105.457,   60.526,   75.839, ..., -999.   , -999.   ,   41.992],\n",
       "       [  94.951,   19.362,   68.812, ..., -999.   , -999.   ,    0.   ],\n",
       "       [-999.   ,   72.756,   70.831, ..., -999.   , -999.   ,    0.   ]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = 'data/train.csv'\n",
    "data_label, data_origin, _ = load_csv_data(data_path, sub_sample=False)\n",
    "data_origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.46141372,   0.24776513,   0.36139276, ...,   0.12369563,\n",
       "          0.11455315,   0.39995606],\n",
       "       [  3.16021405,   0.55250482,   1.52766908, ..., -29.65752746,\n",
       "        -29.65752746,  -0.08526906],\n",
       "       [-26.45649733,   1.98365853,   1.09655998, ..., -26.45649733,\n",
       "        -26.45649733,  -0.90453686],\n",
       "       ...,\n",
       "       [  0.        ,   0.        ,   0.        , ...,   0.        ,\n",
       "          0.        ,   0.        ],\n",
       "       [  0.        ,   0.        ,   0.        , ...,   0.        ,\n",
       "          0.        ,   0.        ],\n",
       "       [  0.        ,   0.        ,   0.        , ...,   0.        ,\n",
       "          0.        ,   0.        ]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_standardized, data_origin_mean ,data_origin_std = standardize(data_origin)\n",
    "data_standardized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1., -1., -1., ...,  1., -1., -1.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/99): loss=1.0, w0=0.009318153994566869, w1=-0.0007455297303470465\n",
      "Gradient Descent(1/99): loss=1.001521834661756, w0=-0.44028494457865924, w1=0.05979904645023791\n",
      "Gradient Descent(2/99): loss=1.2540412441136204, w0=63.15945740994413, w1=-9.010124791954626\n",
      "Gradient Descent(3/99): loss=40.93520140847524, w0=-9872.521571509425, w1=1412.5636097567692\n",
      "Gradient Descent(4/99): loss=6277.334585983553, w0=1551106.8047763654, w1=-221970.3649665808\n",
      "Gradient Descent(5/99): loss=986405.5899601596, w0=-243771461.4621158, w1=34885135.66921705\n",
      "Gradient Descent(6/99): loss=155025848.37737897, w0=38311700937.058, w1=-5482633778.678366\n",
      "Gradient Descent(7/99): loss=24364255517.083767, w0=-6021163757354.643, w1=861664611484.5996\n",
      "Gradient Descent(8/99): loss=3829148216701.045, w0=946301370313379.8, w1=-135421396434733.77\n",
      "Gradient Descent(9/99): loss=601798649502329.8, w0=-1.487231243947206e+17, w1=2.1283170268772584e+16\n",
      "Gradient Descent(10/99): loss=9.458020271027827e+16, w0=2.337370358743748e+19, w1=-3.3449170415908756e+18\n",
      "Gradient Descent(11/99): loss=1.486446463799055e+19, w0=-3.673470562316037e+21, w1=5.2569564937420256e+20\n",
      "Gradient Descent(12/99): loss=2.336136978379101e+21, w0=5.773319543362537e+23, w1=-8.261966211264009e+22\n",
      "Gradient Descent(13/99): loss=3.6715321504427976e+23, w0=-9.073495481819922e+25, w1=1.298471572997177e+25\n",
      "Gradient Descent(14/99): loss=5.770273086079114e+25, w0=1.4260135722655077e+28, w1=-2.0407108704742794e+27\n",
      "Gradient Descent(15/99): loss=9.068707592255015e+27, w0=-2.2411591126703895e+30, w1=3.207232983360001e+29\n",
      "Gradient Descent(16/99): loss=1.4252610953930882e+30, w0=3.5222625268045806e+32, w1=-5.040568734345817e+31\n",
      "Gradient Descent(17/99): loss=2.239976500924966e+32, w0=-5.535677158124378e+34, w1=7.921885718151679e+33\n",
      "Gradient Descent(18/99): loss=3.5204039041788507e+34, w0=8.700010679436833e+36, w1=-1.245023659807316e+36\n",
      "Gradient Descent(19/99): loss=5.53275609964662e+36, w0=-1.367315753073299e+39, w1=1.956710773961614e+38\n",
      "Gradient Descent(20/99): loss=8.69541986981665e+38, w0=2.1489081306776312e+41, w1=-3.0752163003311943e+40\n",
      "Gradient Descent(21/99): loss=1.3665942497850474e+41, w0=-3.3772785428040606e+43, w1=4.83308796561479e+42\n",
      "Gradient Descent(22/99): loss=2.1477741978029822e+43, w0=5.3078166501645577e+45, w1=-7.595803677567272e+44\n",
      "Gradient Descent(23/99): loss=3.375496425126782e+45, w0=-8.341899323581678e+47, w1=1.1937757789352653e+47\n",
      "Gradient Descent(24/99): loss=5.305015828804953e+47, w0=1.311034063744742e+50, w1=-1.8761683040614334e+49\n",
      "Gradient Descent(25/99): loss=8.33749748166718e+49, w0=-2.0604543996834813e+52, w1=2.948633711017548e+51\n",
      "Gradient Descent(26/99): loss=1.3103422591005872e+52, w0=3.2382624148212875e+54, w1=-4.634147556446741e+53\n",
      "Gradient Descent(27/99): loss=2.0593671419514438e+54, w0=-5.089335376145703e+56, w1=7.283143882768116e+55\n",
      "Gradient Descent(28/99): loss=3.2365536529824325e+56, w0=7.998528609769123e+58, w1=-1.1446373722673289e+58\n",
      "Gradient Descent(29/99): loss=5.086649842683037e+58, w0=-1.2570690511212951e+61, w1=1.7989411373445067e+60\n",
      "Gradient Descent(30/99): loss=7.994307957238677e+60, w0=1.9756416165805364e+63, w1=-2.827261536306484e+62\n",
      "Gradient Descent(31/99): loss=1.2564057226604736e+63, w0=-3.104968492926754e+65, w1=4.4433959670728943e+64\n",
      "Gradient Descent(32/99): loss=1.974599112740757e+65, w0=4.8798472664056904e+67, w1=-6.98335384493392e+66\n",
      "Gradient Descent(33/99): loss=3.1033300674405214e+67, w0=-7.669291781122368e+69, w1=1.0975216092586707e+69\n",
      "Gradient Descent(34/99): loss=4.87727227533946e+69, w0=1.205325355752666e+72, w1=-1.7248928087234022e+71\n",
      "Gradient Descent(35/99): loss=7.665244859826975e+71, w0=-1.8943199120371444e+74, w1=2.7108853042040457e+73\n",
      "Gradient Descent(36/99): loss=1.2046893313335555e+74, w0=2.977161238676176e+76, w1=-4.260496127865708e+75\n",
      "Gradient Descent(37/99): loss=1.893320319922108e+76, w0=-4.678982142749111e+78, w1=6.695903816885505e+77\n",
      "Gradient Descent(38/99): loss=2.975590254345356e+78, w0=7.353607056196913e+80, w1=-1.0523452334984754e+80\n",
      "Gradient Descent(39/99): loss=4.676513143913928e+80, w0=-1.1557115433908714e+83, w1=1.6538924703103996e+82\n",
      "Gradient Descent(40/99): loss=7.349726714981188e+82, w0=1.8163455856691947e+85, w1=-2.5992993708498607e+84\n",
      "Gradient Descent(41/99): loss=1.155101699119749e+85, w0=-2.8546148088997514e+87, w1=4.085124843716387e+86\n",
      "Gradient Descent(42/99): loss=1.8153871389390101e+87, w0=4.4863850643199565e+89, w1=-6.420285856989415e+88\n",
      "Gradient Descent(43/99): loss=2.8531084896997523e+89, w0=-7.050916600937469e+91, w1=1.0090284155908157e+91\n",
      "Gradient Descent(44/99): loss=4.484017694845134e+91, w0=1.1081399434203813e+94, w1=-1.585814660201339e+93\n",
      "Gradient Descent(45/99): loss=7.047195983003148e+93, w0=-1.7415808521128403e+96, w1=2.492306557131985e+95\n",
      "Gradient Descent(46/99): loss=1.107555201665432e+96, w0=2.7371126566236027e+98, w1=-3.9169722229295386e+97\n",
      "Gradient Descent(47/99): loss=1.7406618571340052e+98, w0=-4.301715700399595e+100, w1=6.15601293159422e+99\n",
      "Gradient Descent(48/99): loss=2.73566834079704e+100, w0=6.760685542951362e+102, w1=-9.67494612091278e+101\n",
      "Gradient Descent(49/99): loss=4.299445776999687e+102, w0=-1.0625264939388195e+105, w1=1.5205390807768253e+104\n",
      "Gradient Descent(50/99): loss=6.757118073740893e+104, w0=1.6698935975493907e+107, w1=-2.3897178002594448e+106\n",
      "Gradient Descent(51/99): loss=1.061965821425899e+107, w0=-2.624447148418128e+109, w1=3.7557411296257416e+108\n",
      "Gradient Descent(52/99): loss=1.6690124304020406e+109, w0=4.1246477290217445e+111, w1=-5.902618054412549e+110\n",
      "Gradient Descent(53/99): loss=2.6230622837713406e+111, w0=-6.482401026356562e+113, w1=9.276704302500443e+112\n",
      "Gradient Descent(54/99): loss=4.122471240604434e+113, w0=1.01879059321449e+116, w1=-1.4579503861290407e+115\n",
      "Gradient Descent(55/99): loss=6.478980401935488e+115, w0=-1.6011571462521873e+118, w1=2.291351819676821e+117\n",
      "Gradient Descent(56/99): loss=1.0182529992012624e+118, w0=2.516419197497147e+120, w1=-3.601146658684436e+119\n",
      "Gradient Descent(57/99): loss=1.60031224986053e+120, w0=-3.954868260341773e+122, w1=5.659653461327979e+121\n",
      "Gradient Descent(58/99): loss=2.5150913368903096e+122, w0=6.215571305534234e+124, w1=-8.894854983224568e+123\n",
      "Gradient Descent(59/99): loss=3.952781360920021e+124, w0=-9.768549572582206e+126, w1=1.397937978238523e+126\n",
      "Gradient Descent(60/99): loss=6.212291481451738e+126, w0=1.5352500367429084e+129, w1=-2.1970347967305035e+128\n",
      "Gradient Descent(61/99): loss=9.763394917834588e+128, w0=-2.412837911919462e+131, w1=3.4529156322992785e+130\n",
      "Gradient Descent(62/99): loss=1.5344399181237747e+131, w0=3.7920772837413625e+133, w1=-5.4266898191686674e+132\n",
      "Gradient Descent(63/99): loss=2.4115647089423474e+133, w0=-5.959724876184407e+135, w1=8.528723412178759e+134\n",
      "Gradient Descent(64/99): loss=3.79007628563732e+135, w0=9.366454832578712e+137, w1=-1.3403958115407666e+137\n",
      "Gradient Descent(65/99): loss=5.9565800567924114e+137, w0=-1.4720558071617698e+140, w1=2.1066000675206018e+139\n",
      "Gradient Descent(66/99): loss=9.361512354628191e+139, w0=2.3135202572712334e+142, w1=-3.310786117256406e+141\n",
      "Gradient Descent(67/99): loss=1.471279034786428e+142, w0=-3.635987137691557e+144, w1=5.203315467049633e+143\n",
      "Gradient Descent(68/99): loss=2.312299462096962e+144, w0=5.714409642149286e+146, w1=-8.177662612671018e+145\n",
      "Gradient Descent(69/99): loss=3.6340685050202146e+146, w0=-8.980911186341724e+148, w1=1.285222205537273e+148\n",
      "Gradient Descent(70/99): loss=5.711394270361194e+148, w0=1.4114627894723612e+151, w1=-2.019887828395718e+150\n",
      "Gradient Descent(71/99): loss=8.976172151529989e+150, w0=-2.218290733233063e+153, w1=3.174506962082555e+152\n",
      "Gradient Descent(72/99): loss=1.4107179907369113e+153, w0=3.486322001437386e+155, w1=-4.989135688942983e+154\n",
      "Gradient Descent(73/99): loss=2.217120188642518e+155, w0=-5.4791921165319084e+157, w1=7.841052238976713e+156\n",
      "Gradient Descent(74/99): loss=3.484482343858448e+157, w0=8.611237354865023e+159, w1=-1.2323196651199426e+159\n",
      "Gradient Descent(75/99): loss=5.47630086400289e+159, w0=-1.3533639121374463e+162, w1=1.936744853570204e+161\n",
      "Gradient Descent(76/99): loss=8.606693389030147e+161, w0=2.1269810634602828e+164, w1=-3.043837353245177e+163\n",
      "Gradient Descent(77/99): loss=1.352649770937351e+164, w0=-3.3428174076058725e+166, w1=4.783772016190758e+165\n",
      "Gradient Descent(78/99): loss=2.125858701030181e+166, w0=5.253656655698525e+168, w1=-7.518297480150041e+167\n",
      "Gradient Descent(79/99): loss=3.34105347433282e+168, w0=-8.256780102067613e+170, w1=1.1815947082913092e+170\n",
      "Gradient Descent(80/99): loss=5.250884413409999e+170, w0=1.297656510917065e+173, w1=-1.857024224364903e+172\n",
      "Gradient Descent(81/99): loss=8.252423175746367e+172, w0=-2.0394298982284585e+175, w1=2.9185463896203174e+174\n",
      "Gradient Descent(82/99): loss=1.2969717653215116e+175, w0=3.2052197748761326e+177, w1=-4.586861558727855e+176\n",
      "Gradient Descent(83/99): loss=2.0383537346763144e+177, w0=-5.037404724811073e+179, w1=7.20882800895702e+178\n",
      "Gradient Descent(84/99): loss=3.2035284489318903e+179, w0=7.916913080485893e+181, w1=-1.132957700976179e+181\n",
      "Gradient Descent(85/99): loss=5.034746594042785e+181, w0=-1.2442421474545977e+184, w1=1.7805850695929425e+183\n",
      "Gradient Descent(86/99): loss=7.912735494725229e+183, w0=1.9554825293186305e+186, w1=-2.798412674476331e+185\n",
      "Gradient Descent(87/99): loss=1.2435855874765887e+186, w0=-3.0732859598858085e+188, w1=4.398056363833285e+187\n",
      "Gradient Descent(88/99): loss=1.9544506629981762e+188, w0=4.83005419359195e+190, w1=-6.912096974072666e+189\n",
      "Gradient Descent(89/99): loss=3.071664252594856e+190, w0=-7.591035724479734e+192, w1=1.0863226986327816e+192\n",
      "Gradient Descent(90/99): loss=4.827505477265615e+192, w0=1.1930264353302147e+195, w1=-1.7072923166317875e+194\n",
      "Gradient Descent(91/99): loss=7.587030097232234e+194, w0=-1.874990616638507e+197, w1=2.683223924252424e+196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(92/99): loss=1.1923969007884476e+197, w0=2.9467828275820035e+199, w1=-4.217022801276589e+198\n",
      "Gradient Descent(93/99): loss=1.8740012241793724e+199, w0=-4.631238661076639e+201, w1=6.62758003376154e+200\n",
      "Gradient Descent(94/99): loss=2.945227873289195e+201, w0=7.278572188996542e+203, w1=-1.0416072943835537e+203\n",
      "Gradient Descent(95/99): loss=4.628794855455929e+203, w0=-1.1439188732743477e+206, w1=1.6370164527417347e+205\n",
      "Gradient Descent(96/99): loss=7.2747314420759135e+205, w0=1.7978119260965336e+208, w1=-2.572776593440729e+207\n",
      "Gradient Descent(97/99): loss=1.1433152517431062e+208, w0=-2.825486839257492e+210, w1=4.043440973773012e+209\n",
      "Gradient Descent(98/99): loss=1.796863259182237e+210, w0=4.4406068081609896e+212, w1=-6.354774429334104e+211\n",
      "Gradient Descent(99/99): loss=2.823995890264288e+212, w0=-6.978970332018141e+214, w1=9.987324733971887e+213\n"
     ]
    }
   ],
   "source": [
    "w_initial = np.array([0]*data_standardized.shape[1])\n",
    "max_iters = 100\n",
    "gamma = 0.7\n",
    "gradient_losses, gradient_ws = gradient_descent(data_label, data_standardized, w_initial, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD(0/99): loss=1.0109866004025014, w0=0.039129554047094506, w1=-0.003911611855527391\n",
      "SGD(1/99): loss=1.0078669029308651, w0=-0.0014162817532029082, w1=-0.00758540302645439\n",
      "SGD(2/99): loss=3.029700223271565, w0=0.9365843856946237, w1=0.33810032054083716\n",
      "SGD(3/99): loss=573.0891868859952, w0=1368.7460494208303, w1=-178.52671035243654\n",
      "SGD(4/99): loss=257632.79448900843, w0=100456.98195170342, w1=41099.60995691036\n",
      "SGD(5/99): loss=21879330.61875585, w0=-9134061.594259407, w1=-2592091.2901169085\n",
      "SGD(6/99): loss=1853564227.026352, w0=761370686.3262926, w1=223437080.2143623\n",
      "SGD(7/99): loss=673396479.9461037, w0=2186573079.931691, w1=263691391.34726855\n",
      "SGD(8/99): loss=129586321530.7064, w0=-466582697254.6773, w1=51606998622.96219\n",
      "SGD(9/99): loss=22472368489022.008, w0=39104324377330.48, w1=-5153554187914.212\n",
      "SGD(10/99): loss=16851596311330.396, w0=-29397102094357.367, w1=-11471323405876.348\n",
      "SGD(11/99): loss=3179148440140987.0, w0=-1.150687888613272e+16, w1=1244813499082501.2\n",
      "SGD(12/99): loss=1.546905093197404e+18, w0=2.1014347448963018e+18, w1=-3.8569391605501626e+17\n",
      "SGD(13/99): loss=6.505006343949027e+20, w0=-2.342856169641883e+21, w1=1.974654316032433e+20\n",
      "SGD(14/99): loss=8.907209011747883e+19, w0=-1.7359112070273473e+21, w1=1.2246695316558956e+20\n",
      "SGD(15/99): loss=2.923793424228193e+22, w0=2.948818717548296e+22, w1=-5.894398683577806e+21\n",
      "SGD(16/99): loss=2.544595614132653e+22, w0=3.1496873951833404e+22, w1=-5.558465616653927e+21\n",
      "SGD(17/99): loss=2.1056329803609693e+24, w0=9.251748545327741e+23, w1=2.5283587562432445e+23\n",
      "SGD(18/99): loss=2.1055633810646382e+24, w0=9.249660057744848e+23, w1=2.5331573078165314e+23\n",
      "SGD(19/99): loss=2.1040781294044427e+24, w0=9.314300327221615e+23, w1=2.588766280905726e+23\n",
      "SGD(20/99): loss=1.775468273669548e+26, w0=-7.43359809433503e+25, w1=-2.1292022020917365e+25\n",
      "SGD(21/99): loss=1.507605241077759e+28, w0=6.084870561861621e+27, w1=1.8130469528357976e+27\n",
      "SGD(22/99): loss=7.853515990359642e+30, w0=7.398083929903078e+30, w1=-1.474391783541455e+30\n",
      "SGD(23/99): loss=3.1893599089739433e+33, w0=-1.1486830421112319e+34, w1=9.728356736346352e+32\n",
      "SGD(24/99): loss=2.3147064757155706e+33, w0=-1.1088325209976654e+33, w1=1.9575777792873643e+33\n",
      "SGD(25/99): loss=1.0785286862485569e+36, w0=1.4230969445531585e+36, w1=-2.646597036681729e+35\n",
      "SGD(26/99): loss=1.0701342112170785e+39, w0=-2.2775706010515515e+39, w1=2.7843300484364917e+38\n",
      "SGD(27/99): loss=1.0691395879467808e+39, w0=-2.273913102032773e+39, w1=2.8171560683562283e+38\n",
      "SGD(28/99): loss=2.7846731922049867e+41, w0=2.647830489908272e+41, w1=-6.636779377891686e+40\n",
      "SGD(29/99): loss=2.784241446100803e+41, w0=2.6458844726311047e+41, w1=-6.650647752118724e+40\n",
      "SGD(30/99): loss=2.1964811567362622e+41, w0=-4.974793087602838e+41, w1=-1.3560519943657222e+41\n",
      "SGD(31/99): loss=5.802673919094237e+43, w0=-4.874349072555114e+43, w1=1.3455394333460546e+43\n",
      "SGD(32/99): loss=2.419807362398445e+46, w0=8.71580323864076e+46, w1=-7.377821865945972e+45\n",
      "SGD(33/99): loss=1.3735649750255167e+46, w0=4.621394308612179e+46, w1=-5.108396046666733e+45\n",
      "SGD(34/99): loss=2.4069510271248668e+48, w0=-4.1449270964807654e+48, w1=5.275625260744254e+47\n",
      "SGD(35/99): loss=4.882547783065429e+50, w0=1.7616100308911926e+51, w1=-1.9288397351230355e+50\n",
      "SGD(36/99): loss=1.2599897924229203e+53, w0=-1.236985208586007e+53, w1=2.936733542953208e+52\n",
      "SGD(37/99): loss=5.280816636266073e+55, w0=1.89751600913488e+56, w1=-1.605125382633541e+55\n",
      "SGD(38/99): loss=2.7926732545808865e+58, w0=-1.0040277718965441e+59, w1=8.601010185432507e+57\n",
      "SGD(39/99): loss=2.592259133289691e+60, w0=9.395587615769816e+60, w1=-7.845482665115568e+59\n",
      "SGD(40/99): loss=2.231020667402987e+60, w0=7.885149018490486e+60, w1=-6.831375294386238e+59\n",
      "SGD(41/99): loss=3.8982038040494445e+62, w0=-6.769562931800034e+62, w1=8.952180602729141e+61\n",
      "SGD(42/99): loss=3.898171239926512e+62, w0=-6.768993002768786e+62, w1=8.956015480125647e+61\n",
      "SGD(43/99): loss=3.8945330370413944e+62, w0=-6.753828971555887e+62, w1=9.09303312733907e+61\n",
      "SGD(44/99): loss=1.6747840040556992e+62, w0=-7.466642669405641e+62, w1=6.50206309844348e+61\n",
      "SGD(45/99): loss=1.1469029322841737e+62, w0=-7.635506619806988e+62, w1=7.586691361015689e+61\n",
      "SGD(46/99): loss=1.14690887573634e+62, w0=-7.634751017381938e+62, w1=7.593192649039945e+61\n",
      "SGD(47/99): loss=1.14690887573634e+62, w0=-7.634751017381938e+62, w1=7.593192649039945e+61\n",
      "SGD(48/99): loss=1.839649534132137e+61, w0=-6.4776901683025275e+62, w1=6.246889986995342e+61\n",
      "SGD(49/99): loss=1.9393220241552023e+61, w0=-6.4691081895410435e+62, w1=6.260852716427226e+61\n",
      "SGD(50/99): loss=5.8130135240160825e+63, w0=2.0390227270388616e+64, w1=-2.2254573861626343e+63\n",
      "SGD(51/99): loss=9.63731911633184e+65, w0=4.86672598011808e+65, w1=1.8330145741994856e+65\n",
      "SGD(52/99): loss=9.628707912620735e+65, w0=4.894883326907034e+65, w1=1.857984649958105e+65\n",
      "SGD(53/99): loss=9.628707912620735e+65, w0=4.894883326907034e+65, w1=1.857984649958105e+65\n",
      "SGD(54/99): loss=4.755433571097989e+68, w0=1.3846063540155542e+69, w1=-1.29949110646277e+68\n",
      "SGD(55/99): loss=4.355973155849073e+70, w0=-1.5717155359618902e+71, w1=1.3382836324412479e+70\n",
      "SGD(56/99): loss=1.6142870174316295e+73, w0=1.0580426328463717e+73, w1=-2.938444037828724e+72\n",
      "SGD(57/99): loss=1.4506186838863883e+73, w0=1.0806970013280309e+73, w1=-3.0779696539563766e+72\n",
      "SGD(58/99): loss=1.2860943061514357e+75, w0=-4.67217287878146e+75, w1=3.9604421709064474e+74\n",
      "SGD(59/99): loss=7.819115278793955e+74, w0=-4.833812385895096e+75, w1=4.71265517107351e+74\n",
      "SGD(60/99): loss=7.817768267105468e+74, w0=-4.83320719617041e+75, w1=4.716968081179079e+74\n",
      "SGD(61/99): loss=7.809686343393188e+74, w0=-4.8312183102084985e+75, w1=4.735397133940249e+74\n",
      "SGD(62/99): loss=6.918640012238069e+74, w0=-4.4172019817788305e+75, w1=4.4162223262532275e+74\n",
      "SGD(63/99): loss=4.622223367615417e+74, w0=-3.5888021326276446e+75, w1=4.0417111974859207e+74\n",
      "SGD(64/99): loss=9.358036869341321e+76, w0=-6.319714690893688e+76, w1=-6.416162235388644e+75\n",
      "SGD(65/99): loss=1.2245308854496853e+76, w0=-3.6091972175780037e+77, w1=-1.5983367595655847e+76\n",
      "SGD(66/99): loss=1.2456582972739647e+76, w0=-3.6246462153404317e+77, w1=-1.5867533908910956e+76\n",
      "SGD(67/99): loss=2.468367048939e+78, w0=3.2350710337732057e+78, w1=-5.680455330705397e+77\n",
      "SGD(68/99): loss=3.0462597148379516e+77, w0=-5.641500901275613e+78, w1=-1.070963693923295e+78\n",
      "SGD(69/99): loss=6.867601056976386e+79, w0=-2.804235416372087e+79, w1=-1.0756755259450343e+79\n",
      "SGD(70/99): loss=1.9600335892864555e+82, w0=-7.080024893641615e+82, w1=7.303794965472415e+81\n",
      "SGD(71/99): loss=8.385115801135466e+81, w0=-7.257987864797079e+82, w1=7.592984244015659e+81\n",
      "SGD(72/99): loss=2.1226417331069695e+84, w0=2.363761867500185e+84, w1=-5.132741312389572e+83\n",
      "SGD(73/99): loss=2.1207196360890147e+84, w0=2.3566746640563685e+84, w1=-5.196348897853711e+83\n",
      "SGD(74/99): loss=9.220275126186929e+86, w0=-1.1591858080298367e+87, w1=1.8874495058310104e+86\n",
      "SGD(75/99): loss=7.971832212033608e+88, w0=2.891958915220864e+89, w1=-2.455759051426975e+88\n",
      "SGD(76/99): loss=7.459108930850816e+90, w0=-2.6561656349695316e+91, w1=2.2591559599782053e+90\n",
      "SGD(77/99): loss=5.161208338437901e+90, w0=-1.827117289188837e+91, w1=1.8853893216613735e+90\n",
      "SGD(78/99): loss=5.161213736396333e+90, w0=-1.827044146726828e+91, w1=1.886051341259643e+90\n",
      "SGD(79/99): loss=1.5708954148528808e+93, w0=-1.0650393774065484e+93, w1=-9.456826317473648e+91\n",
      "SGD(80/99): loss=7.361411790455253e+95, w0=-9.196604293324332e+95, w1=1.783345911426479e+95\n",
      "SGD(81/99): loss=7.361411790455253e+95, w0=-9.196604293324332e+95, w1=1.783345911426479e+95\n",
      "SGD(82/99): loss=5.20659476988163e+98, w0=1.0421260461564191e+99, w1=-1.2386104933238919e+98\n",
      "SGD(83/99): loss=4.7192189744092915e+100, w0=-1.7184059948107315e+101, w1=1.4327114035637783e+100\n",
      "SGD(84/99): loss=1.3087237515765176e+103, w0=-4.4084899796855645e+102, w1=-1.826672933792818e+102\n",
      "SGD(85/99): loss=1.0016567047817409e+103, w0=-3.297393371350478e+102, w1=-1.7819377426951845e+102\n",
      "SGD(86/99): loss=2.967678722819269e+105, w0=9.638975047821103e+104, w1=4.1641689623327e+104\n",
      "SGD(87/99): loss=8.467253100393914e+107, w0=3.057544275695309e+108, w1=-3.157209445103748e+107\n",
      "SGD(88/99): loss=3.678029570412484e+110, w0=-1.322265587831062e+111, w1=1.1202654918700191e+110\n",
      "SGD(89/99): loss=3.220550791519836e+110, w0=-1.1090956617265411e+111, w1=9.568535683721465e+109\n",
      "SGD(90/99): loss=2.9823697607263275e+110, w0=-1.1167330402234379e+111, w1=9.336502519872464e+109\n",
      "SGD(91/99): loss=9.133766158880535e+112, w0=5.21649165274386e+112, w1=-1.1667714840208102e+112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD(92/99): loss=1.507377038440406e+115, w0=7.370517316240791e+114, w1=2.905388744811981e+114\n",
      "SGD(93/99): loss=5.621791020068657e+117, w0=9.423539078751702e+117, w1=-1.6805454734221958e+117\n",
      "SGD(94/99): loss=5.480935096271205e+117, w0=9.37878874783026e+117, w1=-1.7580863504390987e+117\n",
      "SGD(95/99): loss=5.47805180695649e+117, w0=9.392812052257977e+117, w1=-1.745967300116758e+117\n",
      "SGD(96/99): loss=5.47805180695649e+117, w0=9.392812052257977e+117, w1=-1.745967300116758e+117\n",
      "SGD(97/99): loss=9.312703520954589e+119, w0=4.6037437084192145e+119, w1=1.7771385317217755e+119\n",
      "SGD(98/99): loss=1.6305677013428776e+122, w0=-7.866713408254515e+121, w1=-3.1358353360347573e+121\n",
      "SGD(99/99): loss=3.199422417327611e+124, w0=-1.1580360014909633e+125, w1=1.26446264326181e+124\n"
     ]
    }
   ],
   "source": [
    "w_initial = np.array([0]*data_standardized.shape[1])\n",
    "max_iters = 100\n",
    "gamma = 0.7\n",
    "batch_size = int(data_standardized.shape[0]*0.1)\n",
    "gradient_stochastic_losses, gradient_stochastic_ws  = stochastic_gradient_descent(data_label, data_standardized, w_initial, batch_size, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda=0.000, Training RMSE=1.000\n",
      "lambda=0.000, Training RMSE=1.000\n",
      "lambda=0.000, Training RMSE=1.000\n",
      "lambda=0.000, Training RMSE=1.000\n",
      "lambda=0.000, Training RMSE=1.000\n",
      "lambda=0.000, Training RMSE=1.000\n",
      "lambda=0.000, Training RMSE=1.000\n",
      "lambda=0.000, Training RMSE=1.000\n",
      "lambda=0.000, Training RMSE=1.000\n",
      "lambda=0.000, Training RMSE=1.000\n",
      "lambda=0.000, Training RMSE=1.000\n",
      "lambda=0.000, Training RMSE=1.000\n",
      "lambda=0.000, Training RMSE=1.000\n",
      "lambda=0.000, Training RMSE=1.000\n",
      "lambda=0.000, Training RMSE=1.000\n",
      "lambda=0.000, Training RMSE=1.000\n",
      "lambda=0.000, Training RMSE=1.000\n",
      "lambda=0.000, Training RMSE=1.000\n",
      "lambda=0.000, Training RMSE=1.000\n",
      "lambda=0.000, Training RMSE=1.000\n",
      "lambda=0.000, Training RMSE=1.000\n",
      "lambda=0.000, Training RMSE=1.000\n",
      "lambda=0.000, Training RMSE=1.000\n",
      "lambda=0.000, Training RMSE=1.000\n",
      "lambda=0.000, Training RMSE=1.000\n",
      "lambda=0.000, Training RMSE=1.000\n",
      "lambda=0.000, Training RMSE=1.000\n",
      "lambda=0.000, Training RMSE=1.000\n",
      "lambda=0.000, Training RMSE=1.000\n",
      "lambda=0.001, Training RMSE=1.000\n",
      "lambda=0.002, Training RMSE=1.000\n",
      "lambda=0.004, Training RMSE=1.000\n",
      "lambda=0.012, Training RMSE=1.000\n",
      "lambda=0.029, Training RMSE=1.000\n",
      "lambda=0.075, Training RMSE=1.000\n",
      "lambda=0.193, Training RMSE=1.000\n",
      "lambda=0.494, Training RMSE=1.000\n",
      "lambda=1.265, Training RMSE=1.000\n",
      "lambda=3.237, Training RMSE=1.000\n",
      "lambda=8.286, Training RMSE=1.000\n",
      "lambda=21.210, Training RMSE=1.000\n",
      "lambda=54.287, Training RMSE=1.000\n",
      "lambda=138.950, Training RMSE=1.000\n",
      "lambda=355.648, Training RMSE=1.000\n",
      "lambda=910.298, Training RMSE=1.000\n",
      "lambda=2329.952, Training RMSE=1.000\n",
      "lambda=5963.623, Training RMSE=1.000\n",
      "lambda=15264.180, Training RMSE=1.000\n",
      "lambda=39069.399, Training RMSE=1.000\n",
      "lambda=100000.000, Training RMSE=1.000\n"
     ]
    }
   ],
   "source": [
    "ridge_regression(data_label, data_standardized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse=0.999940049545477\n"
     ]
    }
   ],
   "source": [
    "least_squares_regression(data_label, data_standardized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=173286.79513948376\n"
     ]
    }
   ],
   "source": [
    "max_iters = 10000\n",
    "threshold = 1e-8\n",
    "gamma = 0.01\n",
    "w_initial = np.array([0]*data_standardized.shape[1], dtype='float64')\n",
    "logistic_regression_gradient_descent(data_label, data_standardized, w_initial, max_iters, gamma, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=<built-in function iter>, loss=173286.79513948376\n",
      "Current iteration=<built-in function iter>, loss=176202.51664451216\n",
      "Current iteration=<built-in function iter>, loss=179316.1545544692\n",
      "Current iteration=<built-in function iter>, loss=183546.32960363786\n",
      "Current iteration=<built-in function iter>, loss=188886.32215113725\n",
      "Current iteration=<built-in function iter>, loss=195329.44392368844\n",
      "Current iteration=<built-in function iter>, loss=202869.03788123754\n",
      "Current iteration=<built-in function iter>, loss=211498.4780831341\n",
      "Current iteration=<built-in function iter>, loss=221211.16955486214\n",
      "Current iteration=<built-in function iter>, loss=232000.5481553216\n",
      "Current iteration=<built-in function iter>, loss=243860.08044465838\n",
      "Current iteration=<built-in function iter>, loss=256783.26355264022\n",
      "Current iteration=<built-in function iter>, loss=270763.62504757644\n",
      "Current iteration=<built-in function iter>, loss=285794.7228057795\n",
      "Current iteration=<built-in function iter>, loss=301870.1448815657\n",
      "Current iteration=<built-in function iter>, loss=318983.50937779364\n",
      "Current iteration=<built-in function iter>, loss=337128.46431693714\n",
      "Current iteration=<built-in function iter>, loss=356298.68751269125\n",
      "Current iteration=<built-in function iter>, loss=376487.8864421096\n",
      "Current iteration=<built-in function iter>, loss=397689.7981182693\n",
      "Current iteration=<built-in function iter>, loss=419898.18896346306\n",
      "Current iteration=<built-in function iter>, loss=443106.8546829153\n",
      "Current iteration=<built-in function iter>, loss=467309.6201390202\n",
      "Current iteration=<built-in function iter>, loss=492500.33922610054\n",
      "Current iteration=<built-in function iter>, loss=518672.8947456833\n",
      "Current iteration=<built-in function iter>, loss=545821.1982822923\n",
      "Current iteration=<built-in function iter>, loss=573939.1900797538\n",
      "Current iteration=<built-in function iter>, loss=603020.8389180137\n",
      "Current iteration=<built-in function iter>, loss=633060.1419904651\n",
      "Current iteration=<built-in function iter>, loss=664051.1247817824\n",
      "Current iteration=<built-in function iter>, loss=695987.8409462606\n",
      "Current iteration=<built-in function iter>, loss=728864.3721866604\n",
      "Current iteration=<built-in function iter>, loss=762674.8281335506\n",
      "Current iteration=<built-in function iter>, loss=797413.3462251535\n",
      "Current iteration=<built-in function iter>, loss=833074.0915876854\n",
      "Current iteration=<built-in function iter>, loss=869651.2569161933\n",
      "Current iteration=<built-in function iter>, loss=907139.0623558842\n",
      "Current iteration=<built-in function iter>, loss=945531.7553839464\n",
      "Current iteration=<built-in function iter>, loss=984823.6106918587\n",
      "Current iteration=<built-in function iter>, loss=1025008.9300681883\n",
      "Current iteration=<built-in function iter>, loss=1066082.0422818738\n",
      "Current iteration=<built-in function iter>, loss=1108037.3029659896\n",
      "Current iteration=<built-in function iter>, loss=1150869.0945019946\n",
      "Current iteration=<built-in function iter>, loss=1194571.8259044564\n",
      "Current iteration=<built-in function iter>, loss=1239139.932706259\n",
      "Current iteration=<built-in function iter>, loss=1284567.8768442785\n",
      "Current iteration=<built-in function iter>, loss=1330850.1465455366\n",
      "Current iteration=<built-in function iter>, loss=1377981.2562138268\n",
      "Current iteration=<built-in function iter>, loss=1425955.746316808\n",
      "Current iteration=<built-in function iter>, loss=1474768.1832735643\n",
      "Current iteration=<built-in function iter>, loss=1524413.1593426364\n",
      "Current iteration=<built-in function iter>, loss=1574885.2925105104\n",
      "Current iteration=<built-in function iter>, loss=1626179.2263805731\n",
      "Current iteration=<built-in function iter>, loss=1678289.630062526\n",
      "Current iteration=<built-in function iter>, loss=1731211.1980622574\n",
      "Current iteration=<built-in function iter>, loss=1784938.6501721716\n",
      "Current iteration=<built-in function iter>, loss=1839466.7313619724\n",
      "Current iteration=<built-in function iter>, loss=1894790.2116699002\n",
      "Current iteration=<built-in function iter>, loss=1950903.8860944174\n",
      "Current iteration=<built-in function iter>, loss=2007802.5744863458\n",
      "Current iteration=<built-in function iter>, loss=2065481.1214414518\n",
      "Current iteration=<built-in function iter>, loss=2123934.396193472\n",
      "Current iteration=<built-in function iter>, loss=2183157.29250759\n",
      "Current iteration=<built-in function iter>, loss=2243144.7285743463\n",
      "Current iteration=<built-in function iter>, loss=2303891.6469039978\n",
      "Current iteration=<built-in function iter>, loss=2365393.014221306\n",
      "Current iteration=<built-in function iter>, loss=2427643.821360771\n",
      "Current iteration=<built-in function iter>, loss=2490639.0831622905\n",
      "Current iteration=<built-in function iter>, loss=2554373.8383672605\n",
      "Current iteration=<built-in function iter>, loss=2618843.149515102\n",
      "Current iteration=<built-in function iter>, loss=2684042.1028402173\n",
      "Current iteration=<built-in function iter>, loss=2749965.808169377\n",
      "Current iteration=<built-in function iter>, loss=2816609.3988195304\n",
      "Current iteration=<built-in function iter>, loss=2883968.0314960396\n",
      "Current iteration=<built-in function iter>, loss=2952036.886191337\n",
      "Current iteration=<built-in function iter>, loss=3020811.166084007\n",
      "Current iteration=<built-in function iter>, loss=3090286.0974382786\n",
      "Current iteration=<built-in function iter>, loss=3160456.9295039433\n",
      "Current iteration=<built-in function iter>, loss=3231318.9344166843\n",
      "Current iteration=<built-in function iter>, loss=3302867.4070988204\n",
      "Current iteration=<built-in function iter>, loss=3375097.665160464\n",
      "Current iteration=<built-in function iter>, loss=3448005.048801086\n",
      "Current iteration=<built-in function iter>, loss=3521584.92071149\n",
      "Current iteration=<built-in function iter>, loss=3595832.665976202\n",
      "Current iteration=<built-in function iter>, loss=3670743.6919762534\n",
      "Current iteration=<built-in function iter>, loss=3746313.4282923755\n",
      "Current iteration=<built-in function iter>, loss=3822537.3266085926\n",
      "Current iteration=<built-in function iter>, loss=3899410.8606162188\n",
      "Current iteration=<built-in function iter>, loss=3976929.5259182537\n",
      "Current iteration=<built-in function iter>, loss=4055088.839934173\n",
      "Current iteration=<built-in function iter>, loss=4133884.3418051144\n",
      "Current iteration=<built-in function iter>, loss=4213311.592299461\n",
      "Current iteration=<built-in function iter>, loss=4293366.1737188175\n",
      "Current iteration=<built-in function iter>, loss=4374043.6898043705\n",
      "Current iteration=<built-in function iter>, loss=4455339.765643647\n",
      "Current iteration=<built-in function iter>, loss=4537250.047577655\n",
      "Current iteration=<built-in function iter>, loss=4619770.203108411\n",
      "Current iteration=<built-in function iter>, loss=4702895.920806858\n",
      "Current iteration=<built-in function iter>, loss=4786622.910221156\n",
      "Current iteration=<built-in function iter>, loss=4870946.901785357\n"
     ]
    }
   ],
   "source": [
    "max_iters = 100\n",
    "threshold = 1e-8\n",
    "gamma = 0.01\n",
    "lambda_ = 0.1\n",
    "w_initial = np.array([0]*data_standardized.shape[1], dtype='float64')\n",
    "regularized_logistic_regression(data_label, data_standardized, lambda_, w_initial, max_iters, gamma, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 验证\n",
    "aI = 2 * data_standardized.shape[0] * 10* np.identity(data_standardized.shape[1])\n",
    "a = (data_standardized.T.dot(data_standardized) + aI)\n",
    "\n",
    "w_test.dot(a)\n",
    "\n",
    "np.identity(data_standardized.shape[1]).dot(data_standardized.T.dot(data_label))\n",
    "\n",
    "np.identity(data_standardized.shape[1]).shape\n",
    "\n",
    "np.identity(data_standardized.shape[1]).dot(data_standardized.T.dot(data_label)).shape\n",
    "\n",
    "data_standardized.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
